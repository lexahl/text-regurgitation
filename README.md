# Text Regurgitation

<img src="https://github.com/lexahl/text-regurgitation/blob/main/img/cover.png?raw=true" alt="receipt printer printing 1m long receipt, all surrounding the printer" title="Text Regurgitation" width="200"/>

- - -

## About
*Text Regurgitation* aims to critique Large Language Models often unacknowledged but harmful decontextualization of language. Text Regurgitation is simultaneously is a commentary on western education systems and knowledge production. Regurgitation refers to the act of bringing up something that has been previously swallowed or digested. In the context of information, regurgitation refers to the repetition of previously learned information without understanding it. Language models can not understand; they can only regurgitate without meaning, even if the produced text is seemingly coherent. 

The project takes form as multiple receipts, each containing a “thesis.” These theses have been generated intentionally without using Large Language Models. Instead, the text is generated using various functions that take inspiration from algorithms, some over 100 years old. The text corpus was created from the assigned readings for the course “MA Internet Equalities.” This is the repository for the code that generates the thesis, with created text copora. 

- - -

## Algorithms 

#### Abstract 
* context free grammar
* https://www.nltk.org/

#### Introduction 
* travesty (also markov)
* reference: https://github.com/rodneyshupe/travestypy

#### Lit Review  
* markov word level
* reference https://medium.com/upperlinecode/making-a-markov-chain-poem-generator-in-python-4903d0586957

#### Methods 
* markov character level
* reference https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb

#### Presentation of Work 
* dada word level

#### Discussion - markov character level
* dada character level 

#### Conclusion - scrambled abstract
* context free grammar
* https://www.nltk.org/

- - -

## References

- - -

## Acknowledgements

- - -
